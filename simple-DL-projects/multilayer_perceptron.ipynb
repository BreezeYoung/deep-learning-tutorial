{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7946, train acc 0.704, test acc 0.817\n",
      "epoch 2, loss 0.4872, train acc 0.823, test acc 0.838\n",
      "epoch 3, loss 0.4287, train acc 0.842, test acc 0.860\n",
      "epoch 4, loss 0.3987, train acc 0.852, test acc 0.860\n",
      "epoch 5, loss 0.3770, train acc 0.862, test acc 0.869\n",
      "epoch 6, loss 0.3575, train acc 0.868, test acc 0.875\n",
      "epoch 7, loss 0.3438, train acc 0.873, test acc 0.874\n",
      "epoch 8, loss 0.3314, train acc 0.878, test acc 0.877\n",
      "epoch 9, loss 0.3195, train acc 0.882, test acc 0.883\n",
      "epoch 10, loss 0.3113, train acc 0.885, test acc 0.884\n",
      "epoch 11, loss 0.3018, train acc 0.888, test acc 0.884\n",
      "epoch 12, loss 0.2948, train acc 0.892, test acc 0.887\n",
      "epoch 13, loss 0.2898, train acc 0.893, test acc 0.871\n",
      "epoch 14, loss 0.2830, train acc 0.894, test acc 0.885\n",
      "epoch 15, loss 0.2767, train acc 0.897, test acc 0.887\n",
      "epoch 16, loss 0.2709, train acc 0.900, test acc 0.885\n",
      "epoch 17, loss 0.2631, train acc 0.902, test acc 0.894\n",
      "epoch 18, loss 0.2617, train acc 0.903, test acc 0.889\n",
      "epoch 19, loss 0.2546, train acc 0.906, test acc 0.885\n",
      "epoch 20, loss 0.2486, train acc 0.907, test acc 0.894\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros(num_hiddens)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))\n",
    "b2 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    return nd.maximum(X, 0)\n",
    "\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(nd.dot(X, W1) + b1)\n",
    "    return nd.dot(H, W2) + b2\n",
    "\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs, lr = 20, 0.4\n",
    "\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
